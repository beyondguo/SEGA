{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Ng is an [MASK] and deeplearning.<|endoftext|> <|startofpiece|>  assistant professor of computer science at Stanford University, where his research interests include machine learning, computer vision, natural language processing, <|endofpiece|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"BAAI/glm-10b\", trust_remote_code=True)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"BAAI/glm-10b\", trust_remote_code=True)\n",
    "# model = model.half().cuda()\n",
    "\n",
    "inputs = tokenizer(\"Ng is an [MASK] and deeplearning.\", return_tensors=\"pt\")\n",
    "inputs = tokenizer.build_inputs_for_generation(inputs, max_gen_length=512, mask_id=tokenizer.mask_token_id)\n",
    "inputs = {key: value.cuda() for key, value in inputs.items()}\n",
    "inputs[\"generation_attention_mask\"] = inputs[\"generation_attention_mask\"].half()\n",
    "outputs = model.generate(**inputs, max_length=512, eos_token_id=tokenizer.eop_token_id, num_beams=4)\n",
    "print(tokenizer.decode(outputs[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Loading BAAI/glm-large requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-faf206f0e230>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mglm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text2text-generation'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BAAI/glm-large'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_from_pipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_from_pipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                 raise ValueError(\n\u001b[0;32m--> 656\u001b[0;31m                     \u001b[0;34mf\"Loading {pretrained_model_name_or_path} requires you to execute the configuration file in that repo \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m                     \u001b[0;34m\"on your local machine. Make sure you have read the code there to avoid malicious use, then set \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m                     \u001b[0;34m\"the option `trust_remote_code=True` to remove this error.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Loading BAAI/glm-large requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error."
     ]
    }
   ],
   "source": [
    "glm = pipeline('text2text-generation',model='BAAI/glm-large', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'<mask> Conference on Empirical Methods <mask> submission of research papers <mask> Deep Learning <mask>'\n",
    "'Bad news: <mask> the European Union <mask> month by EU <mask> Farm Commissioner Franz <mask>'\n",
    "\n",
    "ss = ['nice weekend movie fun',\n",
    "      'Shanghai food water help citizens virus',\n",
    "      'England ship China ocean Paris cake people',\n",
    "      'joint great food great drinks greater staff',\n",
    "      'Wuhan hot-dry noodel delicious breakfirst China street Hubuxiang tour nice place',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I am interested in machine learning, and my research interest is in data science. I am a graduate student at the University of California, Davis.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# sega = pipeline(\"text2text-generation\",model='saved_models/bart-base-c4-realnewslike-4templates-passage-max15sents_2-sketch4/checkpoint-129375', framework='pt')\n",
    "s = '<mask> machine learning <mask> my research interest <mask> data science <mask>'\n",
    "sega(s, num_beams=3, do_sample=True, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 09:47:24.890192: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "# sega-chinese\n",
    "from transformers import BertTokenizer, BartForConditionalGeneration, Text2TextGenerationPipeline\n",
    "model_path = 'saved_models/bart-base-chinese-chinese_clean_passages_80m_with_sketch-10000000/checkpoint-93750'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "sega_model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "sega_generator = Text2TextGenerationPipeline(sega_model, tokenizer, device=7)\n",
    "\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('fnlp/bart-base-chinese')\n",
    "bart_generator = Text2TextGenerationPipeline(bart_model, tokenizer, device=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '今 天 的 篮 球 是 上 海 财 经 大 学 篮 球'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_generator(\"今天[MASK]篮球[MASK]上海财经大学[MASK]\", max_length=50, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '今 天 的 篮 球 比 赛 是 由 上 海 财 经 大 学 举 办 的 。'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sega_generator(\"今天[MASK]篮球[MASK]上海财经大学[MASK]\", max_length=50, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketchs = [\n",
    "    # \"今天[MASK]篮球[MASK]学校[MASK]\",\n",
    "    # \"自然语言处理[MASK]谷歌[MASK]通用人工智能[MASK]\",\n",
    "    # \"我昨天做了一个梦[MASK]又遇见了她[MASK]曾经那段时光让人怀恋[MASK]\",\n",
    "    \"[MASK]疫情[MASK]公园[MASK]散步[MASK]\",\n",
    "    # \"[MASK]酸菜鱼火锅[MASK]很美味，味道绝了[MASK]周末真开心[MASK]\"\n",
    "    \"\"\n",
    "]\n",
    "for sketch in sketchs:\n",
    "    print('input sketch:\\n>>> ', sketch)\n",
    "    print('BART-chinese output:\\n>>> ',bart_generator(sketch, max_length=100, do_sample=False)[0]['generated_text'].replace(' ',''))\n",
    "    print('SEGA-chinese output:\\n>>> ',sega_generator(sketch, max_length=100, do_sample=True, num_beams=3)[0]['generated_text'].replace(' ',''),'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_name = 'conll2003'\n",
    "# dataset_name = 'wikiann'\n",
    "raw_datasets = load_dataset(dataset_name)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "sega = pipeline('text2text-generation', model='beyond/sega-base-ps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# 合并多条样本\n",
    "def concat_multiple_sequences(dataset, size=3, overlap=True):\n",
    "    # 传入正经的huggingface dataset格式\n",
    "    # 如果是子集的话，建议使用select方法来筛选\n",
    "    new_dataset = defaultdict(list)\n",
    "    l = len(dataset)\n",
    "    if overlap: # 连续窗口滑动\n",
    "        for i in range(l-size):\n",
    "            concat_tokens = np.concatenate(dataset[i:i+size]['tokens'])\n",
    "            concat_tags = np.concatenate(dataset[i:i+size]['ner_tags'])\n",
    "            new_dataset['tokens'].append(concat_tokens)\n",
    "            new_dataset['ner_tags'].append(concat_tags)\n",
    "    else:  # 互相不重叠\n",
    "        for i in range(l//size):\n",
    "            concat_tokens = np.concatenate(dataset[i*size:(i+1)*size]['tokens'])\n",
    "            concat_tags = np.concatenate(dataset[i*size:(i+1)*size]['ner_tags'])\n",
    "            new_dataset['tokens'].append(concat_tokens)\n",
    "            new_dataset['ner_tags'].append(concat_tags)\n",
    "    return new_dataset\n",
    "\n",
    "\n",
    "tag_names = raw_datasets['train'].features['ner_tags'].feature.names\n",
    "\n",
    "def get_mention_name(tag):\n",
    "    # tag: the number/index of the tag name\n",
    "    # tag_names: the list of tag names\n",
    "    # mention: ORG, LOC, etc.\n",
    "    return tag_names[tag].split('-')[-1]\n",
    "\n",
    "# 单独把实体抽出来\n",
    "def extract_mentions(tokens, tags):\n",
    "    \"\"\"\n",
    "    return: \n",
    "    mentions: []\n",
    "    mention_dict: {'MISC': [], 'PER': [], 'LOC': [], 'ORG': []}\n",
    "    \"\"\"\n",
    "    mentions = []\n",
    "    mention_dict = {t:[] for t in list(set([t.split('-')[-1] for t in tag_names])) if t != 'O'}\n",
    "    for i in range(len(tokens)):\n",
    "        mention = get_mention_name(tags[i])\n",
    "        if mention == 'O':\n",
    "            continue\n",
    "        if tags[i] % 2 == 1:\n",
    "            # the start\n",
    "            mention_dict[mention].append([tokens[i]])\n",
    "            mentions.append([tokens[i]])\n",
    "        else:\n",
    "            # the remaining part\n",
    "            mention_dict[mention][-1].append(tokens[i])\n",
    "            mentions[-1].append(tokens[i])\n",
    "    for k in mention_dict:\n",
    "        if mention_dict[k]: # not empty\n",
    "            mention_dict[k] = [' '.join(items) for items in mention_dict[k]]\n",
    "    mentions = [' '.join(items) for items in mentions]\n",
    "    return mentions,mention_dict\n",
    "    \n",
    "\n",
    "def get_spans(tokens,window=3):\n",
    "    spans = []\n",
    "    for i in range(len(tokens) // window):\n",
    "        spans.append(' '.join(tokens[i*window:(i+1)*window]))\n",
    "    return spans\n",
    "\n",
    "def extract_mention_spans(tokens, tags):\n",
    "    \"\"\"\n",
    "    把一个句子中\"\"\"\n",
    "    text = ' '.join(tokens)\n",
    "    kws = keynotes_yake(text, 2, 5)\n",
    "    spans = get_spans(tokens, window=3)\n",
    "    mentions, _ = extract_mentions(tokens, tags)\n",
    "    wanted_spans = []\n",
    "    for span in spans[1:-1]:\n",
    "        for w in mentions+[]:\n",
    "            if w in span:\n",
    "                wanted_spans.append(span)\n",
    "                break\n",
    "    m = '<mask> ' + ' <mask> '.join(wanted_spans) + ' <mask>'\n",
    "    m = f\"{spans[0]} {m} {spans[-1]}\" # 相当于限制了边界\n",
    "    return wanted_spans, m\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "class MDataset(Dataset):\n",
    "    def __init__(self, m_list):\n",
    "        self.masked_contents = m_list\n",
    "    def __len__(self):\n",
    "        return len(self.masked_contents)\n",
    "    def __getitem__(self, i):\n",
    "        return self.masked_contents[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_data = raw_datasets['train'].select(range(100))\n",
    "orig_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British',\n",
       "       'lamb', '.', 'Peter', 'Blackburn', 'BRUSSELS', '1996-08-22'],\n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat_dataset = concat_multiple_sequences(orig_data)\n",
    "concat_dataset['tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('conda': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}