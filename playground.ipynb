{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "sega = pipeline(\"text2text-generation\",model='beyond/sega-large', device=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'<mask> Conference on Empirical Methods <mask> submission of research papers <mask> Deep Learning <mask>'\n",
    "'Bad news: <mask> the European Union <mask> month by EU <mask> Farm Commissioner Franz <mask>'\n",
    "\n",
    "ss = ['nice weekend movie fun',\n",
    "      'Shanghai food water help citizens virus',\n",
    "      'England ship China ocean Paris cake people',\n",
    "      'joint great food great drinks greater staff',\n",
    "      'Wuhan hot-dry noodel delicious breakfirst China street Hubuxiang tour nice place',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I am interested in machine learning, and my research interest is in data science. I am a graduate student at the University of California, Davis.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# sega = pipeline(\"text2text-generation\",model='saved_models/bart-base-c4-realnewslike-4templates-passage-max15sents_2-sketch4/checkpoint-129375', framework='pt')\n",
    "s = '<mask> machine learning <mask> my research interest <mask> data science <mask>'\n",
    "sega(s, num_beams=3, do_sample=True, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 09:47:24.890192: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "# sega-chinese\n",
    "from transformers import BertTokenizer, BartForConditionalGeneration, Text2TextGenerationPipeline\n",
    "model_path = 'saved_models/bart-base-chinese-chinese_clean_passages_80m_with_sketch-10000000/checkpoint-93750'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "sega_model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "sega_generator = Text2TextGenerationPipeline(sega_model, tokenizer, device=7)\n",
    "\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('fnlp/bart-base-chinese')\n",
    "bart_generator = Text2TextGenerationPipeline(bart_model, tokenizer, device=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '今 天 的 篮 球 是 上 海 财 经 大 学 篮 球'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_generator(\"今天[MASK]篮球[MASK]上海财经大学[MASK]\", max_length=50, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '今 天 的 篮 球 比 赛 是 由 上 海 财 经 大 学 举 办 的 。'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sega_generator(\"今天[MASK]篮球[MASK]上海财经大学[MASK]\", max_length=50, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketchs = [\n",
    "    # \"今天[MASK]篮球[MASK]学校[MASK]\",\n",
    "    # \"自然语言处理[MASK]谷歌[MASK]通用人工智能[MASK]\",\n",
    "    # \"我昨天做了一个梦[MASK]又遇见了她[MASK]曾经那段时光让人怀恋[MASK]\",\n",
    "    \"[MASK]疫情[MASK]公园[MASK]散步[MASK]\",\n",
    "    # \"[MASK]酸菜鱼火锅[MASK]很美味，味道绝了[MASK]周末真开心[MASK]\"\n",
    "    \"\"\n",
    "]\n",
    "for sketch in sketchs:\n",
    "    print('input sketch:\\n>>> ', sketch)\n",
    "    print('BART-chinese output:\\n>>> ',bart_generator(sketch, max_length=100, do_sample=False)[0]['generated_text'].replace(' ',''))\n",
    "    print('SEGA-chinese output:\\n>>> ',sega_generator(sketch, max_length=100, do_sample=True, num_beams=3)[0]['generated_text'].replace(' ',''),'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_name = 'conll2003'\n",
    "# dataset_name = 'wikiann'\n",
    "raw_datasets = load_dataset(dataset_name)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "sega = pipeline('text2text-generation', model='beyond/sega-base-ps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# 合并多条样本\n",
    "def concat_multiple_sequences(dataset, size=3, overlap=True):\n",
    "    # 传入正经的huggingface dataset格式\n",
    "    # 如果是子集的话，建议使用select方法来筛选\n",
    "    new_dataset = defaultdict(list)\n",
    "    l = len(dataset)\n",
    "    if overlap: # 连续窗口滑动\n",
    "        for i in range(l-size):\n",
    "            concat_tokens = np.concatenate(dataset[i:i+size]['tokens'])\n",
    "            concat_tags = np.concatenate(dataset[i:i+size]['ner_tags'])\n",
    "            new_dataset['tokens'].append(concat_tokens)\n",
    "            new_dataset['ner_tags'].append(concat_tags)\n",
    "    else:  # 互相不重叠\n",
    "        for i in range(l//size):\n",
    "            concat_tokens = np.concatenate(dataset[i*size:(i+1)*size]['tokens'])\n",
    "            concat_tags = np.concatenate(dataset[i*size:(i+1)*size]['ner_tags'])\n",
    "            new_dataset['tokens'].append(concat_tokens)\n",
    "            new_dataset['ner_tags'].append(concat_tags)\n",
    "    return new_dataset\n",
    "\n",
    "\n",
    "tag_names = raw_datasets['train'].features['ner_tags'].feature.names\n",
    "\n",
    "def get_mention_name(tag):\n",
    "    # tag: the number/index of the tag name\n",
    "    # tag_names: the list of tag names\n",
    "    # mention: ORG, LOC, etc.\n",
    "    return tag_names[tag].split('-')[-1]\n",
    "\n",
    "# 单独把实体抽出来\n",
    "def extract_mentions(tokens, tags):\n",
    "    \"\"\"\n",
    "    return: \n",
    "    mentions: []\n",
    "    mention_dict: {'MISC': [], 'PER': [], 'LOC': [], 'ORG': []}\n",
    "    \"\"\"\n",
    "    mentions = []\n",
    "    mention_dict = {t:[] for t in list(set([t.split('-')[-1] for t in tag_names])) if t != 'O'}\n",
    "    for i in range(len(tokens)):\n",
    "        mention = get_mention_name(tags[i])\n",
    "        if mention == 'O':\n",
    "            continue\n",
    "        if tags[i] % 2 == 1:\n",
    "            # the start\n",
    "            mention_dict[mention].append([tokens[i]])\n",
    "            mentions.append([tokens[i]])\n",
    "        else:\n",
    "            # the remaining part\n",
    "            mention_dict[mention][-1].append(tokens[i])\n",
    "            mentions[-1].append(tokens[i])\n",
    "    for k in mention_dict:\n",
    "        if mention_dict[k]: # not empty\n",
    "            mention_dict[k] = [' '.join(items) for items in mention_dict[k]]\n",
    "    mentions = [' '.join(items) for items in mentions]\n",
    "    return mentions,mention_dict\n",
    "    \n",
    "\n",
    "def get_spans(tokens,window=3):\n",
    "    spans = []\n",
    "    for i in range(len(tokens) // window):\n",
    "        spans.append(' '.join(tokens[i*window:(i+1)*window]))\n",
    "    return spans\n",
    "\n",
    "def extract_mention_spans(tokens, tags):\n",
    "    \"\"\"\n",
    "    把一个句子中\"\"\"\n",
    "    text = ' '.join(tokens)\n",
    "    kws = keynotes_yake(text, 2, 5)\n",
    "    spans = get_spans(tokens, window=3)\n",
    "    mentions, _ = extract_mentions(tokens, tags)\n",
    "    wanted_spans = []\n",
    "    for span in spans[1:-1]:\n",
    "        for w in mentions+[]:\n",
    "            if w in span:\n",
    "                wanted_spans.append(span)\n",
    "                break\n",
    "    m = '<mask> ' + ' <mask> '.join(wanted_spans) + ' <mask>'\n",
    "    m = f\"{spans[0]} {m} {spans[-1]}\" # 相当于限制了边界\n",
    "    return wanted_spans, m\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "class MDataset(Dataset):\n",
    "    def __init__(self, m_list):\n",
    "        self.masked_contents = m_list\n",
    "    def __len__(self):\n",
    "        return len(self.masked_contents)\n",
    "    def __getitem__(self, i):\n",
    "        return self.masked_contents[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_data = raw_datasets['train'].select(range(100))\n",
    "orig_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British',\n",
       "       'lamb', '.', 'Peter', 'Blackburn', 'BRUSSELS', '1996-08-22'],\n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat_dataset = concat_multiple_sequences(orig_data)\n",
    "concat_dataset['tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}